{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRKmeTvHMom",
        "outputId": "82925117-c105-43b9-bd3a-5e5dfbe297fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# import mount to read files from the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "iCCEUi6o8zFI",
        "outputId": "f8d9494f-0a7a-470d-f639-cb8203fd6823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\r",
            "\u001b[K     |▎                               | 10kB 20.5MB/s eta 0:00:01\r",
            "\u001b[K     |▌                               | 20kB 27.2MB/s eta 0:00:01\r",
            "\u001b[K     |▊                               | 30kB 31.9MB/s eta 0:00:01\r",
            "\u001b[K     |█                               | 40kB 30.5MB/s eta 0:00:01\r",
            "\u001b[K     |█▏                              | 51kB 29.9MB/s eta 0:00:01\r",
            "\u001b[K     |█▍                              | 61kB 31.2MB/s eta 0:00:01\r",
            "\u001b[K     |█▋                              | 71kB 21.1MB/s eta 0:00:01\r",
            "\u001b[K     |█▉                              | 81kB 22.1MB/s eta 0:00:01\r",
            "\u001b[K     |██                              | 92kB 23.0MB/s eta 0:00:01\r",
            "\u001b[K     |██▎                             | 102kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██▌                             | 112kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██▊                             | 122kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███                             | 133kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███▏                            | 143kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███▍                            | 153kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███▋                            | 163kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███▉                            | 174kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████                            | 184kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████▎                           | 194kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████▌                           | 204kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████▊                           | 215kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████                           | 225kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████▏                          | 235kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████▍                          | 245kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████▋                          | 256kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████▉                          | 266kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████                          | 276kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████▎                         | 286kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████▌                         | 296kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████▊                         | 307kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████                         | 317kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████▏                        | 327kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████▍                        | 337kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████▊                        | 348kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████                        | 358kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████▏                       | 368kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████▍                       | 378kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████▋                       | 389kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████▉                       | 399kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████                       | 409kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████▎                      | 419kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████▌                      | 430kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████▊                      | 440kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████                      | 450kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████▏                     | 460kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████▍                     | 471kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████▋                     | 481kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████▉                     | 491kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████                     | 501kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████▎                    | 512kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████▌                    | 522kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████▊                    | 532kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████                    | 542kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████▏                   | 552kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████▍                   | 563kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████▋                   | 573kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████▉                   | 583kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████                   | 593kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████▎                  | 604kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████▌                  | 614kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████▊                  | 624kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████                  | 634kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████▏                 | 645kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████▍                 | 655kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████▋                 | 665kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████▉                 | 675kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████▏                | 686kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████▍                | 696kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████▋                | 706kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████▉                | 716kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████                | 727kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████▎               | 737kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████▌               | 747kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████▊               | 757kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████               | 768kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████▏              | 778kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████▍              | 788kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████▋              | 798kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████▉              | 808kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████              | 819kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████▎             | 829kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████▌             | 839kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████▊             | 849kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████             | 860kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████▏            | 870kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████▍            | 880kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████▋            | 890kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████▉            | 901kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████            | 911kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████▎           | 921kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████▌           | 931kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████▊           | 942kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████           | 952kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▏          | 962kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▍          | 972kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▋          | 983kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████▉          | 993kB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████          | 1.0MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████▎         | 1.0MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████▌         | 1.0MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████▉         | 1.0MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████         | 1.0MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████▎        | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████▌        | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████▊        | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████        | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████▏       | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████▍       | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████▋       | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████▉       | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████       | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████▎      | 1.1MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████▌      | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████▊      | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████      | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▏     | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▍     | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▋     | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████▉     | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████     | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████▎    | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████▌    | 1.2MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████▊    | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████    | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████▏   | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████▍   | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████▋   | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████▉   | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████████   | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████████▎  | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████████▌  | 1.3MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |█████████████████████████████▊  | 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████  | 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████▎ | 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████▌ | 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |██████████████████████████████▊ | 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████ | 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████▏| 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████▍| 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████▋| 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |███████████████████████████████▉| 1.4MB 24.2MB/s eta 0:00:01\r",
            "\u001b[K     |████████████████████████████████| 1.5MB 24.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# To Upgrade NLTK version\n",
        "!pip install -U nltk "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "knLZbk0Y8Ul9"
      },
      "outputs": [],
      "source": [
        "# import libraries to read file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "_XlKBJz08UmD"
      },
      "outputs": [],
      "source": [
        "# read the data sets\n",
        "# encoding='iso-8859-1' is a parameter that can be used when working with text data in Python to specify the character encoding of the text\n",
        "# it's essential to know the character encoding of the data to ensure that the text is decoded correctly\n",
        "# otherwise, incorrect characters or symbols may be displayed or processed\n",
        "summary = pd.read_csv('/content/drive/MyDrive/news_summary.csv', encoding='iso-8859-1')\n",
        "raw = pd.read_csv('/content/drive/MyDrive/news_summary_more.csv', encoding='iso-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shows top 5 rows of summary data  and all the attributes present in the data\n",
        "summary.head()"
      ],
      "metadata": {
        "id": "O9N0hX9oZYtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shows top 5 rows of raw data  and all the attributes present in the data\n",
        "raw.head()"
      ],
      "metadata": {
        "id": "PrOMZLadZYkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQvBjgyt8UmF"
      },
      "outputs": [],
      "source": [
        "# creating a new DataFrame object named pre1 by selecting the first two columns of the raw DataFrame using the iloc method\n",
        "# the .copy() method is used to create a copy of the selected columns instead of referencing the original DataFrame\n",
        "pre1 =  raw.iloc[:,0:2].copy()\n",
        "# pre1['head + text'] = pre1['headlines'].str.cat(pre1['text'], sep =\" \") \n",
        "\n",
        "\n",
        "# creating a new DataFrame object named pre2 by selecting the first six columns of the summary DataFrame using the iloc method\n",
        "# the str.cat() method is used to concatenate five columns (author, date, read_more, text, and ctext) in a specific order with a separator string sep\n",
        "# resulting concatenated string is assigned to the text column of the pre2 DataFrame\n",
        "pre2 = summary.iloc[:,0:6].copy()\n",
        "pre2['text'] = pre2['author'].str.cat(pre2['date'].str.cat(pre2['read_more'].str.cat(pre2['text'].str.cat(pre2['ctext'], sep = \" \"), sep =\" \"),sep= \" \"), sep = \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJKvTL-_8UmG"
      },
      "outputs": [],
      "source": [
        "# creating dataframe and merging the two datasets with the same structure into a single DataFrame,\n",
        "pre = pd.DataFrame()\n",
        "pre['text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n",
        "pre['summary'] = pd.concat([pre1['headlines'],pre2['headlines']],ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "87M8mVa-8UmH",
        "outputId": "daa45c5b-c456-4650-b9a5-b0324be0fb42"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
              "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                            summary\n",
              "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  upGrad learner switches to career in ML & Al w...\n",
              "1  Kunal Shah's credit card bill payment platform...  Delhi techie wins free food from Swiggy for on..."
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVfZEWHF8UmP"
      },
      "outputs": [],
      "source": [
        "# performs a series of regular expression substitutions on each row of the column to remove non-alphabetic characters and standardize the text\n",
        "\n",
        "import re\n",
        "\n",
        "# removes non-alphabetic characters:\n",
        "def text_strip(column):\n",
        "    for row in column:\n",
        "        \n",
        "        \n",
        "        \n",
        "        row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() # remove escape charecters\n",
        "        row=re.sub(\"(\\\\r)\", ' ', str(row)).lower() \n",
        "        row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n",
        "        \n",
        "        row=re.sub(\"(__+)\", ' ', str(row)).lower()   # remove _ if it occors more than one time consecutively\n",
        "        row=re.sub(\"(--+)\", ' ', str(row)).lower()   # remove - if it occors more than one time consecutively\n",
        "        row=re.sub(\"(~~+)\", ' ', str(row)).lower()   # remove ~ if it occors more than one time consecutively\n",
        "        row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower() # remove + if it occors more than one time consecutively\n",
        "        row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower() # remove . if it occors more than one time consecutively\n",
        "        \n",
        "        row=re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() # remove <>()|&©ø\"',;?~*!\n",
        "        \n",
        "        row=re.sub(\"(mailto:)\", ' ', str(row)).lower() # remove mailto:\n",
        "        row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() # remove \\x9* in text\n",
        "        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() # replace INC nums to INC_NUM\n",
        "        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() # replace CM# and CHG# to CM_NUM\n",
        "        \n",
        "        \n",
        "        row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() # remove full stop at end of words(not between)\n",
        "        row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() # remove - at end of words(not between)\n",
        "        row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() # remove : at end of words(not between)\n",
        "        \n",
        "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() # remove any single charecters hanging between 2 spaces\n",
        "        \n",
        "        # replace any url \n",
        "        try:\n",
        "            url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(row))\n",
        "            repl_url = url.group(3)\n",
        "            row = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n",
        "        except:\n",
        "            pass # there might be emails with no url in them\n",
        "        \n",
        "\n",
        "        \n",
        "        row = re.sub(\"(\\s+)\",' ',str(row)).lower() # remove multiple spaces\n",
        "        \n",
        "        # should always be last\n",
        "        # remove any single charecters hanging between 2 spaces\n",
        "        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() \n",
        "\n",
        "        \n",
        "        \n",
        "        yield row\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBcX-lkv8UmR"
      },
      "outputs": [],
      "source": [
        "# applies the \"text_strip\" function to the \"text\" column of the \"pre\" DataFrame \n",
        "# and saves the cleaned text data as object named \"brief_cleaning1\"\n",
        "brief_cleaning1 = text_strip(pre['text'])\n",
        "\n",
        "#applies the same \"text_strip\" function to the \"summary\" column of the \"pre\" DataFrame\n",
        "# and saves the cleaned text data as object named \"brief_cleaning2\"\n",
        "brief_cleaning2 = text_strip(pre['summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msfddO-W8UmV"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) \n",
        "\n",
        "# .pipe() method to speed-up the cleaning process\n",
        "# pipeline method perform  multiple preprocessing steps into a single workflow\n",
        "\n",
        "t = time()\n",
        "\n",
        "# batch size refers to the number of text data points that are processed in a single iteration during training or preprocessin\n",
        "# batch the data points into 5000 and run on all cores for faster preprocessing of text data\n",
        "# the batch size of 5000 specifies that 5000 text data points will be processed in a single iteration of the spaCy pipeline\n",
        "text = [str(doc) for doc in nlp.pipe(brief_cleaning1, batch_size=5000, n_process=-1)]\n",
        "\n",
        "# prints the time taken for the entire cleaning process in minutes\n",
        "# calculated by subtracting the current time from the start time and dividing by 60\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0fkm0tq8UmV",
        "outputId": "4d8b57c1-f84a-486f-bce4-6b11e6c39796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to clean up everything: 35.4 mins\n"
          ]
        }
      ],
      "source": [
        "# batch the data points into 5000 and run on all cores for faster preprocessing of summary data\n",
        "t = time()\n",
        "summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(brief_cleaning2, batch_size=5000, n_process=-1)]\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHFeD5xt8UmX"
      },
      "outputs": [],
      "source": [
        "text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWU_-qlE8UmX"
      },
      "outputs": [],
      "source": [
        "summary[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYdoKApZRGd5"
      },
      "outputs": [],
      "source": [
        "# saving the text file of text.txt\n",
        "# text.txt\" in write mode and assigns the file object to the variable \"f\" \n",
        "# it iterates over each item in the list \"text\" and writes each item to a new line in the file using the \"write()\" method of the file object\n",
        "with open('/content/drive/MyDrive/text.txt', \"w\",encoding = 'utf-8') as f:\n",
        "  for item in text:\n",
        "    f.write(item + '\\n')\n",
        "# saving the text file of summary.txt\n",
        "with open('/content/drive/MyDrive/summary.txt', \"w\",encoding = 'utf-8') as f:\n",
        "  for item in summary:\n",
        "    f.write(item + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug2RSpL9WyUR"
      },
      "outputs": [],
      "source": [
        "# reads the contents of two text files, removes newline characters from each line\n",
        "# and stores the processed content in two separate lists for further processing \n",
        "# encoding = 'utf-8' is used for the text file can be read and written correctly\n",
        "text1 = []\n",
        "summary1 = []\n",
        "text = []\n",
        "summary = []\n",
        "\n",
        "with open('/content/drive/MyDrive/text.txt', \"r\",encoding = 'utf-8') as f:\n",
        "  for line in f.readlines():\n",
        "    text1.append(line)\n",
        "\n",
        "with open('/content/drive/MyDrive/summary.txt', \"r\",encoding = 'utf-8') as f:\n",
        "  for line in f.readlines():\n",
        "    summary1.append(line)\n",
        "\n",
        "for item in text1:\n",
        "  text.append(item.replace('\\n', ''))\n",
        "\n",
        "for item in summary1:\n",
        "  summary.append(item.replace('\\n', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW240FS58UmY"
      },
      "outputs": [],
      "source": [
        "# adding two new columns to a Pandas DataFrame called pre\n",
        "# cleaned_text and cleaned_summary is being created using the pd.Series() function to convert a Python list called text into a Pandas Series object  \n",
        "pre['cleaned_text'] = pd.Series(text)\n",
        "pre['cleaned_summary'] = pd.Series(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrQNadK28UmZ"
      },
      "outputs": [],
      "source": [
        "# creating two empty Python lists\n",
        "# lists are likely to be used to store the length of each text and summary in the dataset\n",
        "text_count = []\n",
        "summary_count = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i_pUmai8UmZ"
      },
      "outputs": [],
      "source": [
        "# calculating the word count for each text and summary in the dataset\n",
        "# and storing these counts in separate lists\n",
        "for sent in pre['cleaned_text']:\n",
        "    text_count.append(len(sent.split()))\n",
        "for sent in pre['cleaned_summary']:\n",
        "    summary_count.append(len(sent.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LknEV1cL8Umb",
        "outputId": "620e188d-fafd-4bde-dae0-934cb0c1bcf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9978234465335472\n"
          ]
        }
      ],
      "source": [
        "# calculates the percentage of summaries \n",
        "# check how much percentage of summary have 0-15 words\n",
        "cnt=0\n",
        "for i in pre['cleaned_summary']:\n",
        "    if(len(i.split())<=15):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(pre['cleaned_summary']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZYukgRp8Umb",
        "outputId": "18d3931e-ffd5-4d4d-818a-5049a12de18e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9578389933440218\n"
          ]
        }
      ],
      "source": [
        "# calculates the percentage of text \n",
        "# check how much percentage of text have 0-70 words\n",
        "cnt=0\n",
        "for i in pre['cleaned_text']:\n",
        "    if(len(i.split())<=100):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(pre['cleaned_text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbGSD9M-r3Ep"
      },
      "outputs": [],
      "source": [
        "# model to summarize the text between 0-15 words for Summary and 0-100 words for Text\n",
        "max_text_len=100\n",
        "max_summary_len=15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UspLHLn18Umc"
      },
      "outputs": [],
      "source": [
        "# select the Summaries and Text between max len defined above\n",
        "\n",
        "cleaned_text =np.array(pre['cleaned_text'])\n",
        "cleaned_summary=np.array(pre['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "post_pre=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "vfHugO5G8Umc",
        "outputId": "0b0c848d-fc2c-4a40-9fc4-d613391aa78c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>saurav kant an alumnus of upgrad and iiit-b pg...</td>\n",
              "      <td>_START_ upgrad learner switches to career in m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>kunal shah credit card bill payment platform c...</td>\n",
              "      <td>_START_ delhi techie wins free food from swigg...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                            summary\n",
              "0  saurav kant an alumnus of upgrad and iiit-b pg...  _START_ upgrad learner switches to career in m...\n",
              "1  kunal shah credit card bill payment platform c...  _START_ delhi techie wins free food from swigg..."
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "post_pre_data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xexWSUUF8Umd"
      },
      "outputs": [],
      "source": [
        "# adding special start-of-sequence (sostok) and end-of-sequence (eostok) \n",
        "# tokens to the beginning and end of each summary in the post_pre DataFrame\n",
        "post_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "R9gDgu5T8Ume",
        "outputId": "5796be18-4d27-40bc-a8a9-c64086558861"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>saurav kant an alumnus of upgrad and iiit-b pg...</td>\n",
              "      <td>sostok _START_ upgrad learner switches to care...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>kunal shah credit card bill payment platform c...</td>\n",
              "      <td>sostok _START_ delhi techie wins free food fro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                            summary\n",
              "0  saurav kant an alumnus of upgrad and iiit-b pg...  sostok _START_ upgrad learner switches to care...\n",
              "1  kunal shah credit card bill payment platform c...  sostok _START_ delhi techie wins free food fro..."
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "post_pre_data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK_q36VP8Umf"
      },
      "outputs": [],
      "source": [
        "# Split the data to TRAIN and VALIDATION sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_val,y_train,y_val=train_test_split(np.array(post_pre['text']),np.array(post_pre['summary']),test_size=0.2,random_state=0,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqSxSJwL8Umf"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "# prepare a tokenizer for reviews on training data\n",
        "#  which creates a dictionary of word index mappings based on the words in the training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO6bMeP48Umf",
        "outputId": "fdc21c53-04bf-4d75-debf-2c077e82097e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 57.91270391131826\n",
            "Total Coverage of rare words: 1.3404923996005096\n"
          ]
        }
      ],
      "source": [
        "# RARE WORD ANALYSIS FOR X i.e 'text'\n",
        "# calculates the percentage of rare words in the vocabulary and the total coverage of those rare words\n",
        "# threshold,the frequency threshold for a word to be considered rare (i.e., if a word occurs less than threshold times in the corpus, it is considered rare)\n",
        "# count,the count of rare words in the vocabulary (i.e., the number of words that occur less than threshold times in the corpus)\n",
        "# total count,the total number of words in the vocabulary.\n",
        "# frequency ,the total frequency of rare words in the corpus (i.e., the sum of the counts of all rare words)\n",
        "# total_freq, the total frequency of all words in the corpu\n",
        "\n",
        "threshold=4\n",
        "\n",
        "count=0\n",
        "total_count=0\n",
        "frequency=0\n",
        "total_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    total_count=total_count+1\n",
        "    total_freq=total_freq+value\n",
        "    if(value<threshold):\n",
        "        count=count+1\n",
        "        frequency=frequency+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(count/total_count)*100)\n",
        "print(\"Total Coverage of rare words:\",(frequency/total_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkb-ZB9E8Umg",
        "outputId": "6face319-cd65-4f0f-a64b-5f543e67e976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in X = 33412\n"
          ]
        }
      ],
      "source": [
        "# prepare a tokenizer for reviews on training data\n",
        "# tokenizer is configured to use only the top total_count - count most frequent words from the text corpus\n",
        "# and any other words will be ignored\n",
        "# this is done to reduce the vocabulary size and prevent overfitting\n",
        "x_tokenizer = Tokenizer(num_words=total_count-count) \n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "# convert text sequences into integer sequences \n",
        "# word in the input text is replaced with an integer, which represents its index in the vocabulary\n",
        "x_train_seq    =   x_tokenizer.texts_to_sequences(x_train) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "# padding zero upto maximum length\n",
        "# pad the integer sequences with zeros up to a fixed length of max_text_len\n",
        "# Padding is necessary because input sequences can have varying lengths,it fixes to fixed length\n",
        "x_train    =   pad_sequences(x_train_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "# size of vocabulary \n",
        "# equal to the number of unique words in the training data ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JZy1TJU8Umh"
      },
      "outputs": [],
      "source": [
        "# prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b59X4dfk8Umi",
        "outputId": "3b73ac3d-56cf-430d-df6f-5798799dec2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 66.34503603813067\n",
            "Total Coverage of rare words: 3.566630093901333\n"
          ]
        }
      ],
      "source": [
        "# RARE WORD ANALYSIS FOR Y i.e 'summary'\n",
        "threshold=6\n",
        "\n",
        "count=0\n",
        "total_count=0\n",
        "frequency=0\n",
        "total_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    total_count=total_count+1\n",
        "    total_freq=total_freq+value\n",
        "    if(value<threshold):\n",
        "        count=count+1\n",
        "        frequency=frequency+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(count/total_count)*100)\n",
        "print(\"Total Coverage of rare words:\",(frequency/total_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivoxCOZh8Umj",
        "outputId": "a72f3964-7205-4f3d-acec-0f08fa776d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in Y = 11581\n"
          ]
        }
      ],
      "source": [
        "# prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=total_count-count) \n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "# convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
        "y_train_seq    =   y_tokenizer.texts_to_sequences(y_train) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "# padding zero upto maximum length\n",
        "y_train   =   pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "# size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y71js0yJ8Umj"
      },
      "outputs": [],
      "source": [
        "# We will now remove \"Summary\" i.e Y (both train and val) which has only START and END\n",
        "ind=[]\n",
        "for i in range(len(y_train)):\n",
        "    count=0\n",
        "    for j in y_train[i]:\n",
        "        if j!=0:\n",
        "            count=count+1\n",
        "    if(count==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_train=np.delete(y_train,ind, axis=0)\n",
        "x_train=np.delete(x_train,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wwrU2uJ8Umk"
      },
      "outputs": [],
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    count=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            count=count+1\n",
        "    if(count==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuz1Y8Ny8Umk",
        "outputId": "de6c5271-46cc-4b3d-f0a2-1567097f3d5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary from the w2v model = 33412\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 200)     6682400     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 100, 300), ( 601200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 100, 300), ( 721200      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    2316200     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 100, 300), ( 721200      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 300),  601200      embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 11581)  3485881     lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 15,129,281\n",
            "Trainable params: 15,129,281\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import backend as K \n",
        "import gensim\n",
        "from numpy import * \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# latent dimension refers to the number of dimensions in a latent space, which is a lower-dimensional space used to represent complex data such as text\n",
        "# embedding dimension is used to represent each item as a vector in an embedding space that captures its properties and characteristics\n",
        "latent_dim = 300\n",
        "embedding_dim=200\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "# embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "# encoder lstm 1\n",
        "# lstm process sequential data and capture the context and dependencies between the elements in the sequence\n",
        "# encoder LSTM learns to encode the input sequence into a fixed-length vector representation\n",
        "# that captures the relevant information from the entire input sequence\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "# encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# embedding layer\n",
        "# creates an embedding layer for the decoder inputs, which will map each token in the target sequence to a vector representation of size embedding_dim\n",
        "# y_voc is the size of the vocabulary of the target sentence\n",
        "# applies the embedding layer to the decoder inputs to obtain a 3D of shape\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# decoder LSTM takes the encoder's output vector as input and generates the output sequence one element at a time, \n",
        "# taking into account the context and dependencies learned by the encoder\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# dense layer\n",
        "# softmax activation function ,we can obtain a probability distribution over the possible output classes/tokens, \n",
        "# which allows us to select the most likely token/class as the next element in the output sequence\n",
        "# dense layer is the final layer ,it is responsible for generating the output sequence.\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGW-CgoJ8Uml"
      },
      "outputs": [],
      "source": [
        "# rmsprop optimizer that adapts the learning rate based on the gradient of the loss function\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53-zmeLa8Uml"
      },
      "outputs": [],
      "source": [
        "# monitor specifies the metric that is being monitored, which in this case is the validation loss \n",
        "# we want to minimize the validation loss, so we set mode to 'min'\n",
        "# verbose argument can be useful for monitoring the progress of training (if verbose =0 no output will be printed during training if it is\n",
        "#                                                                          1 means that progress updates will be printed to the console after each epoch)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HneesAu8Umm",
        "outputId": "563b6798-b67f-469b-ae24-ecd8f93e449c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "692/692 [==============================] - 801s 1s/step - loss: 5.4439 - val_loss: 4.7884\n",
            "Epoch 2/50\n",
            "692/692 [==============================] - 779s 1s/step - loss: 4.7358 - val_loss: 4.4829\n",
            "Epoch 3/50\n",
            "692/692 [==============================] - 777s 1s/step - loss: 4.4383 - val_loss: 4.2456\n",
            "Epoch 4/50\n",
            "692/692 [==============================] - 789s 1s/step - loss: 4.1943 - val_loss: 4.0797\n",
            "Epoch 5/50\n",
            "692/692 [==============================] - 781s 1s/step - loss: 4.0054 - val_loss: 3.9319\n",
            "Epoch 6/50\n",
            "692/692 [==============================] - 780s 1s/step - loss: 3.8555 - val_loss: 3.8262\n",
            "Epoch 7/50\n",
            "692/692 [==============================] - 779s 1s/step - loss: 3.7431 - val_loss: 3.7470\n",
            "Epoch 8/50\n",
            "692/692 [==============================] - 769s 1s/step - loss: 3.6495 - val_loss: 3.6826\n",
            "Epoch 9/50\n",
            "692/692 [==============================] - 767s 1s/step - loss: 3.5531 - val_loss: 3.6252\n",
            "Epoch 10/50\n",
            "692/692 [==============================] - 777s 1s/step - loss: 3.4722 - val_loss: 3.5719\n",
            "Epoch 11/50\n",
            "692/692 [==============================] - 774s 1s/step - loss: 3.4132 - val_loss: 3.5300\n",
            "Epoch 12/50\n",
            "692/692 [==============================] - 769s 1s/step - loss: 3.3524 - val_loss: 3.5063\n",
            "Epoch 13/50\n",
            "692/692 [==============================] - 788s 1s/step - loss: 3.3007 - val_loss: 3.4674\n",
            "Epoch 14/50\n",
            "692/692 [==============================] - 764s 1s/step - loss: 3.2460 - val_loss: 3.4497\n",
            "Epoch 15/50\n",
            "692/692 [==============================] - 763s 1s/step - loss: 3.1966 - val_loss: 3.4275\n",
            "Epoch 16/50\n",
            "692/692 [==============================] - 756s 1s/step - loss: 3.1552 - val_loss: 3.4047\n",
            "Epoch 17/50\n",
            "692/692 [==============================] - 769s 1s/step - loss: 3.1137 - val_loss: 3.3909\n",
            "Epoch 18/50\n",
            "692/692 [==============================] - 768s 1s/step - loss: 3.0815 - val_loss: 3.3713\n",
            "Epoch 19/50\n",
            "692/692 [==============================] - 752s 1s/step - loss: 3.0523 - val_loss: 3.3674\n",
            "Epoch 20/50\n",
            "692/692 [==============================] - 760s 1s/step - loss: 3.0117 - val_loss: 3.3499\n",
            "Epoch 21/50\n",
            "692/692 [==============================] - 756s 1s/step - loss: 2.9796 - val_loss: 3.3469\n",
            "Epoch 22/50\n",
            "692/692 [==============================] - 759s 1s/step - loss: 2.9475 - val_loss: 3.3248\n",
            "Epoch 23/50\n",
            "692/692 [==============================] - 775s 1s/step - loss: 2.9192 - val_loss: 3.3189\n",
            "Epoch 24/50\n",
            "692/692 [==============================] - 788s 1s/step - loss: 2.8967 - val_loss: 3.3114\n",
            "Epoch 25/50\n",
            "692/692 [==============================] - 776s 1s/step - loss: 2.8618 - val_loss: 3.3032\n",
            "Epoch 26/50\n",
            "692/692 [==============================] - 764s 1s/step - loss: 2.8435 - val_loss: 3.3075\n",
            "Epoch 27/50\n",
            "692/692 [==============================] - 779s 1s/step - loss: 2.8251 - val_loss: 3.3028\n",
            "Epoch 28/50\n",
            "692/692 [==============================] - 775s 1s/step - loss: 2.8028 - val_loss: 3.3014\n",
            "Epoch 29/50\n",
            "692/692 [==============================] - 768s 1s/step - loss: 2.7826 - val_loss: 3.2952\n",
            "Epoch 30/50\n",
            "692/692 [==============================] - 756s 1s/step - loss: 2.7688 - val_loss: 3.2964\n",
            "Epoch 31/50\n",
            "692/692 [==============================] - 783s 1s/step - loss: 2.7499 - val_loss: 3.2921\n",
            "Epoch 32/50\n",
            "692/692 [==============================] - 793s 1s/step - loss: 2.7351 - val_loss: 3.2929\n",
            "Epoch 33/50\n",
            "692/692 [==============================] - 769s 1s/step - loss: 2.7182 - val_loss: 3.2912\n",
            "Epoch 34/50\n",
            "692/692 [==============================] - 808s 1s/step - loss: 2.7077 - val_loss: 3.2933\n",
            "Epoch 35/50\n",
            "692/692 [==============================] - 785s 1s/step - loss: 2.6894 - val_loss: 3.3004\n",
            "Epoch 00035: early stopping\n"
          ]
        }
      ],
      "source": [
        "# fitting the model with the data\n",
        "history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:] ,epochs=00,callbacks=[es],batch_size=128, \n",
        "                  validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL1wqNnGvZZU"
      },
      "outputs": [],
      "source": [
        "# saving the model weights which helps to reduce the storage space\n",
        "# also we can access this without accessing the original training data \n",
        "model.save_weights('/content/drive/MyDrive/model/fitted_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9e23ZBRwHwM"
      },
      "outputs": [],
      "source": [
        "model = model.load_weights('/content/drive/MyDrive/model/fitted_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiVIhTtV8Ump"
      },
      "outputs": [],
      "source": [
        "# dictionaries that map the integer word IDs to their corresponding words in the target and source \n",
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rM-GrwH8Umq"
      },
      "outputs": [],
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "# the hidden state is a vector that summarizes the relevant information from the previous time steps, and it is used to inform the output at the current time step\n",
        "# cell state is also a vector that is updated at each time step based on the current input and the previous cell state\n",
        "# encoder outputs contains the final output of the LSTM layer, which contains information from the entire input sequence\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# creates an input layer for the decoder's hidden state vector h, with a shape of (latent_dim,)\n",
        "# creates an input layer for the decoder's cell state vector c, also with a shape of (latent_dim,)\n",
        "# enable the decoder to take in the encoded input text and generate a sequence of output tokens one at a time\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "# dec_emb_layer takes in decoder inputs (usually a sequence of integers representing words or tokens) and outputs their corresponding embeddings\n",
        "# decoder embedding layer can be used as inputs to the decoder layers, which can then generate a sequence of output tokens based on the encoded input sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)  #(word with the highest probability in this distribution is selected as the predicted output at each time step)\n",
        "\n",
        "# Final decoder model\n",
        "# model takes an input sequence and generates an output sequence using the initial hidden and cell states, and updates these states during the decoding process\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25q9dnBe8Umr"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # input sequence is encoded using the encoder model, which returns the encoded output, hidden state, and cell state\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # An empty target sequence of length 1 is created, \n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # and the first word is set to the start token 'sostok'\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        # decoder model predicts the next output token, given the current target sequence, encoded input, and previous hidden and cell states\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c]) \n",
        "\n",
        "        # Sample a token\n",
        "        # predicted token with the highest probability is selected and added to the decoded sentence\n",
        "        # If the selected token is not the end token, it is added to the decoded sentence and the loop continues\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # If the selected token is the end token or the maximum summary length is reached, the loop stops and return decoded sentence\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        # Internal states are used to generate the next word in the sequence using the previously generated words and the current hidden state of the model\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLsShB9l8Umr"
      },
      "outputs": [],
      "source": [
        "# takes an input sequence as an argument and returns a string that represents a summary of the input sequence\n",
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp5cNJsz8Ums",
        "outputId": "48649577-0e95-4126-b773-52d44631012c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: pope francis on tuesday called for respect for each ethnic group in speech delivered in myanmar avoiding reference to the rohingya minority community as the nation works to restore peace the healing of wounds must be priority he said the pope myanmar visit comes amid the country military crackdown resulting in the rohingya refugee crisis \n",
            "Original summary:  pope avoids mention of rohingyas in key myanmar speech  \n",
            "Predicted summary:   pope calls for rohingya muslims in myanmar \n",
            "\n",
            "\n",
            "Review: students of government school in uttar pradesh sambhal were seen washing dishes at in school premises on being approached basic shiksha adhikari virendra pratap singh said yes have also received this complaint from elsewhere we are inquiring and action will be taken against those found guilty \n",
            "Original summary:  students seen washing dishes at govt school in up  \n",
            "Predicted summary:   up school students protest up school in up \n",
            "\n",
            "\n",
            "Review: apple india profit surged by 140 in 2017 18 to crore compared to ã¢ââ¹373 crore in the previous fiscal the indian unit of the us based company posted 12 growth in revenue last fiscal at ã¢ââ¹13 crore apple share of the indian smartphone market dropped to 1 in the second quarter of 2018 according to counterpoint research \n",
            "Original summary:  apple india profit rises 140 to nearly ã¢ââ¹900 crore in fy18  \n",
            "Predicted summary:   apple profit rises 20 to ã¢ââ¹3 crore in march quarter \n",
            "\n",
            "\n",
            "Review: uber has launched its electric scooter service in santa monica us at 1 to unlock and then 15 cents per minute to ride it comes after uber acquired the bike sharing startup jump for reported amount of 200 million uber said it is branding the scooters with jump for the sake of consistency for its other personal electric vehicle services \n",
            "Original summary:  uber launches electric scooter service in us at 1 per ride  \n",
            "Predicted summary:   uber launches self driving car service in us city \n",
            "\n",
            "\n",
            "Review: around 80 people were injured in accidents related to kite flying during celebrations of makar sankranti in rajasthan jaipur officials said the victims included those who fell while flying kites and those injured by glass coated kite string officials added meanwhile around 100 birds were reported to be injured by between january 13 and 15 \n",
            "Original summary:  80 people injured in flying related accidents in jaipur  \n",
            "Predicted summary:   people killed in drone in kolkata \n",
            "\n",
            "\n",
            "Review: uk entrepreneur richard browning has announced the launch of his startup gravity which has created flight jet powered suit that will be priced at about ã¢ââ¹1 3 crore the suit has custom built exoskeleton with six attached micro jet engines fuelled by kerosene from backpack browning claims the can travel at speed of up to 450 kmph \n",
            "Original summary:  up makes ã¢ââ¹1 3 crore jet powered flying suit  \n",
            "Predicted summary:   up makes us airport that can be flying plane \n",
            "\n",
            "\n",
            "Review: andhra pradesh chief minister chandrababu naidu on monday announced that his government will provide 100 units free power to most backward classes he added that the government would also give aid of up to ã¢ââ¹15 lakh to backward classes for foreign education we will spread out the poverty eradication program under pro basis he further said n \n",
            "Original summary:  most backward classes to get 100 units free power andhra cm  \n",
            "Predicted summary:   will free 100 free education for andhra cm \n",
            "\n",
            "\n",
            "Review: taking dig at pm modi congress president rahul gandhi tweeted while our pm around his garden making yoga videos india leads afghanistan syria in rape violence against women this comes after thomson reuters foundation survey declared india as world most dangerous country for women pm modi shared video of himself doing yoga and other exercises last week \n",
            "Original summary:  pm modi makes yoga videos while india leads in rape rahul  \n",
            "Predicted summary:   pm modi is the country of india rahul gandhi \n",
            "\n",
            "\n",
            "Review: external affairs minister sushma swaraj on saturday called upon the united nations to pass the comprehensive convention on international terrorism to end pakistan sponsored terrorism proposed by india in 1996 aims to arrive at universal definition of terrorism ban all terror groups prosecute terrorists under special laws and make cross border terrorism an offence \n",
            "Original summary:  india calls on un to pass global anti terror convention  \n",
            "Predicted summary:   india calls for pak to un terror attacks on un \n",
            "\n",
            "\n",
            "Review: the 23 richest indians in the 500 member bloomberg billionaires index saw wealth erosion of 21 billion this year lakshmi mittal who controls the world largest steelmaker arcelormittal lost 5 6 billion or 29 of his net worth followed by sun pharma founder dilip shanghvi whose wealth declined 4 6 billion asia richest person mukesh ambani added 4 billion to his fortune \n",
            "Original summary:  lakshmi mittal lost 10 bn in 2018 ambani added 4 bn  \n",
            "Predicted summary:   which are the richest indians in the world richest person \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prints out the results of these three functions for each of the first 100 training examples\n",
        "for i in range(0,10):\n",
        "    print(\"Review:\",seq2text(x_train[i]))\n",
        "    print(\"Original summary:\",(seq2summary(y_train[i])).replace('start', '').replace('end', ''))\n",
        "    print(\"Predicted summary:\",(decode_sequence(x_train[i].reshape(1,max_text_len))).replace('start', '').replace('end', ''))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0xFJjdRtRnq",
        "outputId": "8cda6766-5cb9-4f4b-c812-5b456eee6da4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 47,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EE2PHb2u9By",
        "outputId": "18199731-2700-4904-c894-616c20d5c4e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.gleu_score import sentence_gleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "def calculate_scores(N=100):    \n",
        "    bscore=0;gscore=0;mscore=0\n",
        "    for i in range(N):\n",
        "        ref=seq2summary(y_train[i])\n",
        "        hypo=decode_sequence(x_train[i].reshape(1,max_text_len))\n",
        "        bscore+=sentence_bleu([ref],hypo)\n",
        "        gscore+=sentence_gleu([ref],hypo)\n",
        "        mscore+=meteor_score([ref],hypo)\n",
        "    print(\"BLEU:%.4f GLEU:%.4f METEOR:%.4f\"%(bscore/N,gscore/N,mscore/N))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51X9S1IACFw",
        "outputId": "6b69c34d-6eb9-45dc-d296-5a6e545e1371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU:0.4279 GLEU:0.4700 METEOR:0.3422\n"
          ]
        }
      ],
      "source": [
        "calculate_scores()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-aeCIUJ-Z8so"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}